---
title: 爬虫
date: 2021-07-27 07:53:19
permalink: /pages/6f3909/
categories:
  - 插件开发
  - 快速入门
tags:
  -
---

> 爬虫功能可以用来爬取html类型的数据，一般用来获取网站的信息，比如爬取豆瓣的数据。。

目前爬虫使用的是go的`colly`框架，感兴趣的可以看一下官方文档

[gocolly/colly: Elegant Scraper and Crawler Framework for Golang (github.com)](https://github.com/gocolly/colly)

> 记得在auth里面加上 `spider`

本次我们实战的对象为一个壁纸网站，地址：[4K壁纸_4K电脑壁纸_4K高清壁纸下载_4K,5K,6K,7K,8K壁纸图片素材_彼岸图网 (netbian.com)](https://pic.netbian.com/)

下面我将通过插件来实现壁纸的爬取操作

![](https://img.xiaoyou66.com/2021/07/29/35b8de7e96f46.png)

## 初始化

使用初始化来初始化一个爬虫对象，后面我们可以通过这个对象来对网站进行爬取操作

```javascript
let colly = spider.init({"Host":"www.douban.com","Cookie":cookie})
```

`init`方法只有一个参数，就是head信息，通过head信息，我们可以设置`cookie`，`host`等头部字段。

我们不需要设置`agent`，初始化的时候我会帮你设置`agent`为谷歌浏览器的`agent`

## 开始爬取

首先我们看一下网站的结构，网站的结构比较简单，我们需要的结果都在`slist`这个类里面

![image-20210729085256538](https://img.xiaoyou66.com/2021/07/29/c2031224bf8c1.png)

`colly`框架使用的是`jquery`选择器，在使用前请参考一下官方文档：[Selectors | jQuery API Documentation](https://api.jquery.com/category/selectors/)

下面这个是我们的插件代码，详细的可以看代码注释

```javascript
// 获取API接口
const router =xBlog.router
const spider = xBlog.spider
// 注册路由
router.registerRouter("GET","",function(context){
    // 所有的图片数据
    let imgs = []
    // 插入数据
    let colly = spider.init({"referer": "https://pic.netbian.com/"})
    // 开始爬取，这里我们从slist这个类开始
    // .slist>.clearfix 表示获取.slist下的.clearfix这个dom的内容(.是css选择器)
    colly.OnHTML(".slist>.clearfix",function (e){
        // 遍历所有的项目，这里我们遍历所有的li
        e.ForEach("li", function (i,element) {
            // 这里我们可以使用element.ChildAttr来获取子元素的属性，比如这里我获取图片地址
            let src = "https://pic.netbian.com" + element.ChildAttr("a>img","src")
            // 我们还能获取标签里的内容
            let text = element.ChildText("a>b")
            // 放到数组中去
            imgs.push({src,text})
        })
    })
    // 开始进行爬取，这里我们使用Visit来访问我们要爬取的网站
    colly.Visit("https://pic.netbian.com/4kdongman/")
    router.response.ResponseOk(context,imgs)
})

```

注意`element`这个对象其实还是可以使用`ForEach`这个函数的，相当于可以无限套娃。。



效果如下，可以看到我们已经把所有的图片都获取到了。下面的那个文本乱码是因为这个网站是GBK编码的，目前暂不支持GBK网页的中文字体获取，这里只是为了演示一下获取标签内容功能而已

![image-20210729222454793](https://img.xiaoyou66.com/2021/07/29/dcdf3718e50da.png)

